# current woring on this file


import sys
import io
import os
import torch
import json
import re
from diffusers import StableDiffusionPipeline, StableVideoDiffusionPipeline
from diffusers.utils import export_to_video
from transformers import pipeline
from PIL import Image
import fitz  # pymupdf
from docx import Document

# --- Fix for Windows Unicode output errors ---
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


def extract_text_from_pdf(path):
    doc = fitz.open(path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text


def extract_text_from_docx(path):
    doc = Document(path)
    return "\n".join([para.text for para in doc.paragraphs])


def load_text_from_file(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    if ext == '.pdf':
        return extract_text_from_pdf(file_path)
    elif ext == '.docx':
        return extract_text_from_docx(file_path)
    else:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()


def generate_story_video(file_path, content_id):
    try:
        print("Loading and extracting text from input file...")
        file_content = load_text_from_file(file_path)

        if len(file_content) > 1500:
            print("Warning: Story is very long. Processing first 1500 characters only.")
            file_content = file_content[:1500]

        print("Generating story script from text using LLM...")

        llm_pipeline = pipeline(
            "text-generation",
            model="EleutherAI/gpt-j-6B",
            device=0 if torch.cuda.is_available() else -1,
            torch_dtype=torch.float16 if torch.cuda.is_available() else None,
        )

        llm_prompt = (
            "You are a helpful assistant that breaks a story into 3-5 scenes.\n"
            "For each scene, output a JSON object with a single key 'prompt' that contains a detailed, cinematic, text-to-image description.\n"
            "Return ONLY a JSON array of these objects and nothing else. No explanations, no extra text.\n"
            f"Story:\n{file_content}\n"
        )

        script_output = llm_pipeline(llm_prompt, max_new_tokens=512, do_sample=True, pad_token_id=llm_pipeline.tokenizer.eos_token_id)
        raw_output = script_output[0]['generated_text']

        print("Raw LLM output:")
        print(raw_output)

        # --- FIX: Refine JSON extraction with a more robust regex ---
        # This regex looks for the first '[' and the last ']' in the output,
        # which should capture the entire JSON array even if there is
        # leading or trailing text.
        json_match = re.search(r'\[.*\]', raw_output.strip(), re.DOTALL)
        
        if json_match:
            json_string = json_match.group(0)
            
            # --- FIX: Attempt to clean up common LLM output issues ---
            # Remove any trailing commas that might exist before the closing bracket
            json_string = re.sub(r',\s*]', ']', json_string)
            
            try:
                scenes = json.loads(json_string)
            except json.JSONDecodeError as e:
                print(f"Failed to parse cleaned JSON: {e}", file=sys.stderr)
                print(f"Problematic JSON string: {json_string}", file=sys.stderr)
                raise
        else:
            raise ValueError("Failed to find a valid JSON array in LLM output.")
        
        if not scenes:
            raise ValueError("No scenes generated by LLM")

        print("Story script generated successfully.")

        print("Loading video generation models...")
        image_model_path = os.path.join(os.path.dirname(__file__), 'models', 'juggernautXL_v9.safetensors')
        image_pipeline = StableDiffusionPipeline.from_single_file(
            image_model_path,
            torch_dtype=torch.float16,
            use_safetensors=True
        )
        svd_pipeline = StableVideoDiffusionPipeline.from_pretrained(
            "stabilityai/stable-video-diffusion-img2vid-xt",
            torch_dtype=torch.float16,
            variant="fp16"
        )

        device = "cuda" if torch.cuda.is_available() else "cpu"
        image_pipeline.to(device)
        svd_pipeline.to(device)

    except Exception as e:
        print(f"Error loading models or generating script: {e}", file=sys.stderr)
        sys.exit(1)

    all_video_frames = []
    for i, scene in enumerate(scenes):
        try:
            print(f"Generating image for scene {i+1}: {scene['prompt']}")
            starting_image = image_pipeline(
                scene['prompt'],
                negative_prompt="blurry, low quality, distorted, bad anatomy",
                num_inference_steps=30,
                guidance_scale=9.0
            ).images[0]
            starting_image = starting_image.resize((1024, 576))
            
            # 
            
            print("Generating video frames...")
            video_frames = svd_pipeline(
                starting_image,
                num_frames=25,
                decode_chunk_size=8,
                motion_bucket_id=140,
                noise_aug_strength=0.01
            ).frames[0]

            all_video_frames.extend(video_frames)

        except Exception as e:
            print(f"Error generating video for scene {i+1}: {e}", file=sys.stderr)
            continue

    if not all_video_frames:
        print("No video frames were generated.", file=sys.stderr)
        sys.exit(1)

    output_dir = os.path.join(os.path.dirname(__file__), '..', 'storage', 'videos')
    os.makedirs(output_dir, exist_ok=True)
    output_filename = f"story_video_{content_id}.mp4"
    output_path = os.path.join(output_dir, output_filename)

    try:
        print("Stitching scenes together into a final video...")
        export_to_video(all_video_frames, output_path, fps=8)
        print(f"storage/videos/{output_filename}") # The app.js script expects this path to be printed to stdout
    except Exception as e:
        print(f"Error saving final video: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    if len(sys.argv) > 2:
        file_path_arg = sys.argv[1]
        content_id_arg = sys.argv[2]
        generate_story_video(file_path_arg, content_id_arg)
    else:
        print("Usage: python story_to_video_gen.py <file_path> <content_id>", file=sys.stderr)
        sys.exit(1)








# import sys
# import os
# import torch
# import json
# from diffusers import StableDiffusionPipeline, StableVideoDiffusionPipeline
# from diffusers.utils import export_to_video
# from transformers import pipeline
# from PIL import Image
# import fitz  # pymupdf
# from docx import Document

# def extract_text_from_pdf(path):
#     doc = fitz.open(path)
#     text = ""
#     for page in doc:
#         text += page.get_text()
#     return text

# def extract_text_from_docx(path):
#     doc = Document(path)
#     return "\n".join([para.text for para in doc.paragraphs])

# def load_text_from_file(file_path):
#     ext = os.path.splitext(file_path)[1].lower()
#     if ext == '.pdf':
#         return extract_text_from_pdf(file_path)
#     elif ext == '.docx':
#         return extract_text_from_docx(file_path)
#     else:
#         with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
#             return f.read()

# def generate_story_video(file_path, content_id):
#     try:
#         print("Loading and extracting text from input file...")
#         file_content = load_text_from_file(file_path)

#         if len(file_content) > 1500:
#             print("Warning: Story is very long. Processing first 1500 characters only.")
#             file_content = file_content[:1500]

#         print("Generating story script from text using LLM...")
#         llm_pipeline = pipeline(
#             "text-generation", 
#             model="microsoft/phi-2",
#             device=0 if torch.cuda.is_available() else -1,
#             dtype=torch.float16
#         )

#         llm_prompt = (
#             "### Instruction:\n"
#             "Break down the following story into 3-5 distinct scenes. "
#             "For each scene, provide a detailed text-to-image prompt. "
#             "Ensure the prompts are descriptive and follow a cinematic style. "
#             "Return ONLY a JSON array of objects, each with a 'prompt' key. NO other text.\n"
#             f"### Story:\n{file_content}\n### Response:\n"
#         )

#         script_output = llm_pipeline(llm_prompt, max_new_tokens=512, do_sample=True)
#         raw_output = script_output[0]['generated_text']

#         json_start = raw_output.find('[')
#         json_end = raw_output.rfind(']') + 1
#         if json_start != -1 and json_end != -1:
#             json_string = raw_output[json_start:json_end]
#             scenes = json.loads(json_string)
#         else:
#             raise ValueError("Failed to parse JSON from LLM output")

#         if not scenes:
#             raise ValueError("No scenes generated by LLM")

#         print("Story script generated successfully.")

#         print("Loading video generation models...")
#         image_model_path = os.path.join(os.path.dirname(__file__), 'models', 'juggernautXL_v9.safetensors')
#         image_pipeline = StableDiffusionPipeline.from_single_file(
#             image_model_path,
#             dtype=torch.float16,
#             use_safetensors=True
#         )
#         svd_pipeline = StableVideoDiffusionPipeline.from_pretrained(
#             "stabilityai/stable-video-diffusion-img2vid-xt",
#             dtype=torch.float16,
#             variant="fp16"
#         )
        
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         image_pipeline.to(device)
#         svd_pipeline.to(device)
        
#     except Exception as e:
#         print(f"Error loading models or generating script: {e}", file=sys.stderr)
#         sys.exit(1)

#     all_video_frames = []
#     for i, scene in enumerate(scenes):
#         try:
#             print(f"Generating image for scene {i+1}: {scene['prompt']}")
#             starting_image = image_pipeline(
#                 scene['prompt'],
#                 negative_prompt="blurry, low quality, distorted, bad anatomy",
#                 num_inference_steps=30,
#                 guidance_scale=9.0
#             ).images[0]
#             starting_image = starting_image.resize((1024, 576))

#             print("Generating video frames...")
#             video_frames = svd_pipeline(
#                 starting_image,
#                 num_frames=25,
#                 decode_chunk_size=8,
#                 motion_bucket_id=140,
#                 noise_aug_strength=0.01
#             ).frames[0]
            
#             all_video_frames.extend(video_frames)
            
#         except Exception as e:
#             print(f"Error generating video for scene {i+1}: {e}", file=sys.stderr)
#             continue

#     if not all_video_frames:
#         print("No video frames were generated.", file=sys.stderr)
#         sys.exit(1)

#     output_dir = os.path.join(os.path.dirname(__file__), '..', 'storage', 'videos')
#     os.makedirs(output_dir, exist_ok=True)
#     output_filename = f"story_video_{content_id}.mp4"
#     output_path = os.path.join(output_dir, output_filename)
    
#     try:
#         print("Stitching scenes together into a final video...")
#         export_to_video(all_video_frames, output_path, fps=8)
#         print(f"Final video generated: storage/videos/{output_filename}")
#     except Exception as e:
#         print(f"Error saving final video: {e}", file=sys.stderr)
#         sys.exit(1)

# if __name__ == "__main__":
#     if len(sys.argv) > 2:
#         file_path_arg = sys.argv[1]
#         content_id_arg = sys.argv[2]
#         generate_story_video(file_path_arg, content_id_arg)
#     else:
#         print("Usage: python story_to_video_gen.py <file_path> <content_id>", file=sys.stderr)
#         sys.exit(1)

# import sys
# import os
# import torch
# import json
# from diffusers import StableDiffusionPipeline, StableVideoDiffusionPipeline
# from diffusers.utils import export_to_video
# from PIL import Image
# from transformers import pipeline

# def generate_story_video(file_path, content_id):
#     try:
#         # Step 1: Use LLM to generate a structured script from the file
#         print("Generating story script from file using LLM...")
#         llm_pipeline = pipeline(
#             "text-generation", 
#             model="microsoft/phi-2",
#             device=0,
#             dtype=torch.float16
#         )

#         with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
#             file_content = f.read()

#         # The model will not be able to process the entire story in a single pass if it is too long.
#         # This approach ensures the story fits within the model's token limit.
#         if len(file_content) > 1500:
#             print("Warning: Story is very long. Processing first 1500 characters only.")
#             file_content = file_content[:1500]

#         llm_prompt = (
#             "### Instruction:\n"
#             "Break down the following story into 3-5 distinct scenes. "
#             "For each scene, provide a detailed text-to-image prompt. "
#             "Ensure the prompts are descriptive and follow a cinematic style. "
#             "Return ONLY a JSON array of objects, each with a 'prompt' key. NO other text.\n"
#             f"### Story:\n{file_content}\n### Response:\n"
#         )

#         # llm_prompt = (
#         #     f"### Instruction:\nBreak down the following story into 3-5 distinct scenes. "
#         #     f"For each scene, provide a detailed text-to-image prompt. "
#         #     f"Ensure the prompts are descriptive and follow a cinematic style. "
#         #     f"Provide the output as a JSON array of objects, with each object "
#         #     f"containing a 'prompt' key.\n### Story:\n{file_content}\n### Response:\n"
#         # )

#         script_output = llm_pipeline(llm_prompt, max_new_tokens=512, do_sample=False)
#         # --- FIX: Robust JSON parsing to handle conversational LLM output ---
#         raw_output = script_output[0]['generated_text']
#         try:
#             # Find the first and last brackets to isolate the JSON content
#             json_start = raw_output.find('[')
#             json_end = raw_output.rfind(']') + 1
#             if json_start != -1 and json_end != -1:
#                 json_string = raw_output[json_start:json_end]
#                 scenes = json.loads(json_string)
#             else:
#                 sys.stderr.write("LLM failed to generate a valid JSON script.\n")
#                 sys.stderr.write(f"LLM raw output:\n{raw_output}\n")
#                 sys.exit(1)
#         except (json.JSONDecodeError, IndexError) as e:
#             sys.stderr.write(f"Error parsing JSON from LLM: {e}\n")
#             sys.stderr.write(f"LLM raw output:\n{raw_output}\n")
#             sys.exit(1)
#         # --- END OF FIX ---
#         if not scenes or not all(isinstance(scene, dict) and 'prompt' in scene for scene in scenes):
#             sys.stderr.write("Parsed JSON script is invalid or missing 'prompt' keys.\n")
#             sys.exit(1)
#         # if not scenes:
#         #     sys.stderr.write("LLM failed to generate a valid script from the file.\n")
#         #     sys.exit(1)
        
#         print("Story script generated successfully.")

#         # Step 2: Load the video generation pipelines
#         print("Loading video generation models...")
#         image_model_path = os.path.join(os.path.dirname(__file__), 'models', 'juggernautXL_v9.safetensors')
#         image_pipeline = StableDiffusionPipeline.from_single_file(
#             image_model_path,
#             dtype=torch.float16,
#             use_safetensors=True
#         )
#         svd_pipeline = StableVideoDiffusionPipeline.from_pretrained(
#             "stabilityai/stable-video-diffusion-img2vid-xt",
#             dtype=torch.float16,
#             variant="fp16"
#         )
        
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         image_pipeline.to(device)
#         svd_pipeline.to(device)
        
#     except Exception as e:
#         sys.stderr.write(f"Error loading models or generating script: {e}\n")
#         sys.exit(1)

#     all_video_frames = []
#     for i, scene in enumerate(scenes):
#         try:
#             print(f"Generating image for scene {i+1}: {scene['prompt']}")
#             starting_image = image_pipeline(
#                 scene['prompt'],
#                 negative_prompt="blurry, low quality, distorted, bad anatomy",
#                 num_inference_steps=30,
#                 guidance_scale=9.0
#             ).images[0]
#             starting_image = starting_image.resize((1024, 576))

#             print("Generating video frames...")
#             video_frames = svd_pipeline(
#                 starting_image,
#                 num_frames=25,
#                 decode_chunk_size=8,
#                 motion_bucket_id=140,
#                 noise_aug_strength=0.01
#             ).frames[0]
            
#             all_video_frames.extend(video_frames)
            
#         except Exception as e:
#             sys.stderr.write(f"Error generating video for scene {i+1}: {e}\n")
#             continue

#     if not all_video_frames:
#         sys.stderr.write("No video frames were generated.\n")
#         sys.exit(1)

#     output_dir = os.path.join(os.path.dirname(__file__), '..', 'storage', 'videos')
#     os.makedirs(output_dir, exist_ok=True)
#     output_filename = f"story_video_{content_id}.mp4"
#     output_path = os.path.join(output_dir, output_filename)
    
#     try:
#         print("Stitching scenes together into a final video...")
#         export_to_video(all_video_frames, output_path, fps=8)
#         print(f"Final video generation complete: storage/videos/{output_filename}")
#         print(f"storage/videos/{output_filename}")
#     except Exception as e:
#         sys.stderr.write(f"Error saving final video: {e}\n")
#         sys.exit(1)

# if __name__ == "__main__":
#     if len(sys.argv) > 2:
#         file_path_arg = sys.argv[1]
#         content_id_arg = sys.argv[2]
#         generate_story_video(file_path_arg, content_id_arg)
#     else:
#         sys.stderr.write("Usage: python story_to_video_gen.py <file_path> <content_id>\n")
#         sys.exit(1)



# # ml_scripts/story_to_video_gen.py
# import sys
# import os
# import torch
# import json
# from diffusers import StableDiffusionPipeline, StableVideoDiffusionPipeline
# from diffusers.utils import export_to_video
# from PIL import Image
# from transformers import pipeline

# def generate_story_video(file_path, content_id):
#     try:
#         # Step 1: Use LLM to generate a structured script from the file
#         print("Generating story script from file using LLM...")
#         llm_pipeline = pipeline(
#             "text-generation", 
#             model="microsoft/phi-2",
#             device=0,
#             torch_dtype=torch.float16
#         )

#         with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
#             file_content = f.read()

#         # The model will not be able to process the entire story in a single pass if it is too long.
#         # This approach ensures the story fits within the model's token limit.
#         if len(file_content) > 1500:
#             print("Warning: Story is very long. Processing first 1500 characters only.")
#             file_content = file_content[:1500]

#         llm_prompt = (
#             f"### Instruction:\nBreak down the following story into 3-5 distinct scenes. "
#             f"For each scene, provide a detailed text-to-image prompt. "
#             f"Ensure the prompts are descriptive and follow a cinematic style. "
#             f"Provide the output as a JSON array of objects, with each object "
#             f"containing a 'prompt' key.\n### Story:\n{file_content}\n### Response:\n"
#         )

#         script_output = llm_pipeline(llm_prompt, max_new_tokens=512, do_sample=True)
#         # --- FIX: Robust JSON parsing to handle conversational LLM output ---
#         raw_output = script_output[0]['generated_text']
#         try:
#             # Find the first and last brackets to isolate the JSON content
#             json_start = raw_output.find('[')
#             json_end = raw_output.rfind(']') + 1
#             if json_start != -1 and json_end != -1:
#                 json_string = raw_output[json_start:json_end]
#                 scenes = json.loads(json_string)
#             else:
#                 sys.stderr.write("LLM failed to generate a valid JSON script.\n")
#                 sys.exit(1)
#         except (json.JSONDecodeError, IndexError) as e:
#             sys.stderr.write(f"Error parsing JSON from LLM: {e}\n")
#             sys.exit(1)
#         # --- END OF FIX ---
        
#         if not scenes:
#             sys.stderr.write("LLM failed to generate a valid script from the file.\n")
#             sys.exit(1)
        
#         print("Story script generated successfully.")

#         # Step 2: Load the video generation pipelines
#         print("Loading video generation models...")
#         image_model_path = os.path.join(os.path.dirname(__file__), 'models', 'juggernautXL_v9.safetensors')
#         image_pipeline = StableDiffusionPipeline.from_single_file(
#             image_model_path,
#             torch_dtype=torch.float16,
#             use_safetensors=True
#         )
#         svd_pipeline = StableVideoDiffusionPipeline.from_pretrained(
#             "stabilityai/stable-video-diffusion-img2vid-xt",
#             torch_dtype=torch.float16,
#             variant="fp16"
#         )
        
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         image_pipeline.to(device)
#         svd_pipeline.to(device)
        
#     except Exception as e:
#         sys.stderr.write(f"Error loading models or generating script: {e}\n")
#         sys.exit(1)

#     all_video_frames = []
#     for i, scene in enumerate(scenes):
#         try:
#             print(f"Generating image for scene {i+1}: {scene['prompt']}")
#             starting_image = image_pipeline(
#                 scene['prompt'],
#                 negative_prompt="blurry, low quality, distorted, bad anatomy",
#                 num_inference_steps=30,
#                 guidance_scale=9.0
#             ).images[0]
#             starting_image = starting_image.resize((1024, 576))

#             print("Generating video frames...")
#             video_frames = svd_pipeline(
#                 starting_image,
#                 num_frames=25,
#                 decode_chunk_size=8,
#                 motion_bucket_id=140,
#                 noise_aug_strength=0.01
#             ).frames[0]
            
#             all_video_frames.extend(video_frames)
            
#         except Exception as e:
#             sys.stderr.write(f"Error generating video for scene {i+1}: {e}\n")
#             continue

#     if not all_video_frames:
#         sys.stderr.write("No video frames were generated.\n")
#         sys.exit(1)

#     output_dir = os.path.join(os.path.dirname(__file__), '..', 'storage', 'videos')
#     os.makedirs(output_dir, exist_ok=True)
#     output_filename = f"story_video_{content_id}.mp4"
#     output_path = os.path.join(output_dir, output_filename)
    
#     try:
#         print("Stitching scenes together into a final video...")
#         export_to_video(all_video_frames, output_path, fps=8)
#         print(f"Final video generation complete: storage/videos/{output_filename}")
#         print(f"storage/videos/{output_filename}")
#     except Exception as e:
#         sys.stderr.write(f"Error saving final video: {e}\n")
#         sys.exit(1)

# if __name__ == "__main__":
#     if len(sys.argv) > 2:
#         file_path_arg = sys.argv[1]
#         content_id_arg = sys.argv[2]
#         generate_story_video(file_path_arg, content_id_arg)
#     else:
#         sys.stderr.write("Usage: python story_to_video_gen.py <file_path> <content_id>\n")
#         sys.exit(1)




# # ml_scripts/story_to_video_gen.py
# import sys
# import os
# import torch
# import json
# from diffusers import StableDiffusionPipeline, StableVideoDiffusionPipeline
# from diffusers.utils import export_to_video
# from PIL import Image
# from transformers import pipeline

# def generate_story_video(file_path, content_id):
#     try:
#         # Step 1: Use LLM to generate a structured script from the file
#         print("Generating story script from file using LLM...")
#         llm_pipeline = pipeline(
#             "text-generation", 
#             model="microsoft/phi-2",
#             device=0,
#             torch_dtype=torch.float16
#         )

#         with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
#             file_content = f.read()

#         llm_prompt = (
#             f"### Instruction:\nBreak down the following story into 3-5 distinct scenes. "
#             f"For each scene, provide a detailed text-to-image prompt. "
#             f"Ensure the prompts are descriptive and follow a cinematic style. "
#             f"Provide the output as a JSON array of objects, with each object "
#             f"containing a 'prompt' key.\n### Story:\n{file_content}\n### Response:\n"
#         )

#         script_output = llm_pipeline(llm_prompt, max_new_tokens=512, do_sample=True)
#         # --- FIX: Robust JSON parsing to handle conversational LLM output ---
#         raw_output = script_output[0]['generated_text']
#         try:
#             # Find the first and last brackets to isolate the JSON content
#             json_start = raw_output.find('[')
#             json_end = raw_output.rfind(']') + 1
#             if json_start != -1 and json_end != -1:
#                 json_string = raw_output[json_start:json_end]
#                 scenes = json.loads(json_string)
#             else:
#                 sys.stderr.write("LLM failed to generate a valid JSON script.\n")
#                 sys.exit(1)
#         except (json.JSONDecodeError, IndexError) as e:
#             sys.stderr.write(f"Error parsing JSON from LLM: {e}\n")
#             sys.exit(1)
#         # --- END OF FIX ---
        
#         if not scenes:
#             sys.stderr.write("LLM failed to generate a valid script from the file.\n")
#             sys.exit(1)
        
#         print("Story script generated successfully.")

#         # Step 2: Load the video generation pipelines
#         print("Loading video generation models...")
#         image_model_path = os.path.join(os.path.dirname(__file__), 'models', 'juggernautXL_v9.safetensors')
#         image_pipeline = StableDiffusionPipeline.from_single_file(
#             image_model_path,
#             torch_dtype=torch.float16,
#             use_safetensors=True
#         )
#         svd_pipeline = StableVideoDiffusionPipeline.from_pretrained(
#             "stabilityai/stable-video-diffusion-img2vid-xt",
#             torch_dtype=torch.float16,
#             variant="fp16"
#         )
        
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         image_pipeline.to(device)
#         svd_pipeline.to(device)
        
#     except Exception as e:
#         sys.stderr.write(f"Error loading models or generating script: {e}\n")
#         sys.exit(1)

#     all_video_frames = []
#     for i, scene in enumerate(scenes):
#         try:
#             # Generate a realistic starting image for the scene
#             print(f"Generating image for scene {i+1}: {scene['prompt']}")
#             starting_image = image_pipeline(
#                 scene['prompt'],
#                 negative_prompt="blurry, low quality, distorted, bad anatomy",
#                 num_inference_steps=30,
#                 guidance_scale=9.0
#             ).images[0]
            
#             # SVD works best with 1024x576 (landscape)
#             starting_image = starting_image.resize((1024, 576))

#             # Generate video frames for the scene
#             print("Generating video frames...")
#             video_frames = svd_pipeline(
#                 starting_image,
#                 num_frames=25,
#                 decode_chunk_size=8,
#                 motion_bucket_id=140,
#                 noise_aug_strength=0.01
#             ).frames[0]
            
#             all_video_frames.extend(video_frames)
            
#         except Exception as e:
#             sys.stderr.write(f"Error generating video for scene {i+1}: {e}\n")
#             continue

#     if not all_video_frames:
#         sys.stderr.write("No video frames were generated.\n")
#         sys.exit(1)

#     # Step 3: Combine all frames into a single video file
#     output_dir = os.path.join(os.path.dirname(__file__), '..', 'storage', 'videos')
#     os.makedirs(output_dir, exist_ok=True)
#     output_filename = f"story_video_{content_id}.mp4"
#     output_path = os.path.join(output_dir, output_filename)
    
#     try:
#         print("Stitching scenes together into a final video...")
#         export_to_video(all_video_frames, output_path, fps=8)
#         print(f"Final video generation complete: storage/videos/{output_filename}")
#         print(f"storage/videos/{output_filename}")
#     except Exception as e:
#         sys.stderr.write(f"Error saving final video: {e}\n")
#         sys.exit(1)

# if __name__ == "__main__":
#     if len(sys.argv) > 2:
#         file_path_arg = sys.argv[1]
#         content_id_arg = sys.argv[2]
#         generate_story_video(file_path_arg, content_id_arg)
#     else:
#         sys.stderr.write("Usage: python story_to_video_gen.py <file_path> <content_id>\n")
#         sys.exit(1)











# # ml_scripts/story_to_video_gen.py
# import sys
# import os
# import torch
# import json
# from diffusers import StableDiffusionPipeline, StableVideoDiffusionPipeline
# from diffusers.utils import export_to_video
# from PIL import Image
# from transformers import pipeline

# def generate_story_video(file_path, content_id):
#     try:
#         # Step 1: Use LLM to generate a structured script from the file
#         print("Generating story script from file using LLM...")
#         llm_pipeline = pipeline(
#             "text-generation", 
#             model="microsoft/phi-2",
#             device=0,
#             torch_dtype=torch.float16
#         )

#         with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
#             file_content = f.read()

#         # The model can't handle long input. Break the content into smaller chunks.
#         # This is a key step to overcoming the model's token limit.
#         max_chunk_size = 1500
#         content_chunks = [file_content[i:i+max_chunk_size] for i in range(0, len(file_content), max_chunk_size)]
        
#         scenes = []
#         for chunk in content_chunks:
#             llm_prompt = (
#                 f"### Instruction:\nBreak down the following story into 3-5 distinct scenes. "
#                 f"For each scene, provide a detailed text-to-image prompt. "
#                 f"Ensure the prompts are descriptive and follow a cinematic style. "
#                 f"Provide the output as a JSON array of objects, with each object "
#                 f"containing a 'prompt' key.\n### Story:\n{chunk}\n### Response:\n"
#             )
#             script_output = llm_pipeline(llm_prompt, max_new_tokens=512, do_sample=True)
#             # --- START OF FIX: Robust JSON parsing to handle conversational LLM output ---
#             raw_output = script_output[0]['generated_text']
#             try:
#                 # Find the first and last brackets to isolate the JSON content
#                 json_start = raw_output.find('[')
#                 json_end = raw_output.rfind(']') + 1
#                 if json_start != -1 and json_end != -1:
#                     json_string = raw_output[json_start:json_end]
#                     scenes.extend(json.loads(json_string))
#                 else:
#                     sys.stderr.write("LLM failed to generate a valid JSON script.\n")
#                     continue
#             except (json.JSONDecodeError, IndexError) as e:
#                 sys.stderr.write(f"Error parsing JSON from LLM: {e}\n")
#                 continue
#             # --- END OF FIX ---

#         if not scenes:
#             sys.stderr.write("LLM failed to generate a valid script from the file.\n")
#             sys.exit(1)
        
#         print("Story script generated successfully.")

#         # Step 2: Load the video generation pipelines
#         print("Loading video generation models...")
#         image_model_path = os.path.join(os.path.dirname(__file__), 'models', 'juggernautXL_v9.safetensors')
#         image_pipeline = StableDiffusionPipeline.from_single_file(
#             image_model_path,
#             torch_dtype=torch.float16,
#             use_safetensors=True
#         )
#         svd_pipeline = StableVideoDiffusionPipeline.from_pretrained(
#             "stabilityai/stable-video-diffusion-img2vid-xt",
#             torch_dtype=torch.float16,
#             variant="fp16"
#         )
        
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         image_pipeline.to(device)
#         svd_pipeline.to(device)
        
#     except Exception as e:
#         sys.stderr.write(f"Error loading models or generating script: {e}\n")
#         sys.exit(1)

#     all_video_frames = []
#     for i, scene in enumerate(scenes):
#         try:
#             # Generate a realistic starting image for the scene
#             print(f"Generating image for scene {i+1}: {scene['prompt']}")
#             starting_image = image_pipeline(
#                 scene['prompt'],
#                 negative_prompt="blurry, low quality, distorted, bad anatomy",
#                 num_inference_steps=30,
#                 guidance_scale=9.0
#             ).images[0]
            
#             # SVD works best with 1024x576 (landscape)
#             starting_image = starting_image.resize((1024, 576))

#             # Generate video frames for the scene
#             print("Generating video frames...")
#             video_frames = svd_pipeline(
#                 starting_image,
#                 num_frames=25,
#                 decode_chunk_size=8,
#                 motion_bucket_id=140,
#                 noise_aug_strength=0.01
#             ).frames[0]
            
#             all_video_frames.extend(video_frames)
            
#         except Exception as e:
#             sys.stderr.write(f"Error generating video for scene {i+1}: {e}\n")
#             continue

#     if not all_video_frames:
#         sys.stderr.write("No video frames were generated.\n")
#         sys.exit(1)

#     # Step 3: Combine all frames into a single video file
#     output_dir = os.path.join(os.path.dirname(__file__), '..', 'storage', 'videos')
#     os.makedirs(output_dir, exist_ok=True)
#     output_filename = f"story_video_{content_id}.mp4"
#     output_path = os.path.join(output_dir, output_filename)
    
#     try:
#         print("Stitching scenes together into a final video...")
#         export_to_video(all_video_frames, output_path, fps=8)
#         print(f"Final video generation complete: storage/videos/{output_filename}")
#         print(f"storage/videos/{output_filename}") # Final path for Node.js
#     except Exception as e:
#         sys.stderr.write(f"Error saving final video: {e}\n")
#         sys.exit(1)

# if __name__ == "__main__":
#     if len(sys.argv) > 2:
#         file_path_arg = sys.argv[1]
#         content_id_arg = sys.argv[2]
#         generate_story_video(file_path_arg, content_id_arg)
#     else:
#         sys.stderr.write("Usage: python story_to_video_gen.py <file_path> <content_id>\n")
#         sys.exit(1)












# # ml_scripts/story_to_video_gen.py
# import sys
# import os
# import torch
# import json
# from diffusers import StableDiffusionPipeline, StableVideoDiffusionPipeline
# from diffusers.utils import export_to_video
# from PIL import Image
# from transformers import pipeline

# def generate_story_video(file_path, content_id):
#     try:
#         # Step 1: Use LLM to generate a structured script from the file
#         print("Generating story script from file using LLM...")
#         llm_pipeline = pipeline(
#             "text-generation", 
#             model="microsoft/phi-2",
#             device=0,
#             torch_dtype=torch.float16
#         )

#         with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
#             file_content = f.read()

#         # The model can't handle long input. Break the content into smaller chunks.
#         # This is a key step to overcoming the model's token limit.
#         max_chunk_size = 1500
#         content_chunks = [file_content[i:i+max_chunk_size] for i in range(0, len(file_content), max_chunk_size)]
        
#         scenes = []
#         for chunk in content_chunks:
#             llm_prompt = (
#                 f"### Instruction:\nBreak down the following story into 3-5 distinct scenes. "
#                 f"For each scene, provide a detailed text-to-image prompt. "
#                 f"Ensure the prompts are descriptive and follow a cinematic style. "
#                 f"Provide the output as a JSON array of objects, with each object "
#                 f"containing a 'prompt' key.\n### Story:\n{chunk}\n### Response:\n"
#             )
#             script_output = llm_pipeline(llm_prompt, max_new_tokens=512, do_sample=True)
#             try:
#                 scenes.extend(json.loads(script_output[0]['generated_text'].split("### Response:")[1].strip()))
#             except (json.JSONDecodeError, IndexError) as e:
#                 sys.stderr.write(f"Error parsing JSON from LLM: {e}\n")
#                 continue

#         if not scenes:
#             sys.stderr.write("LLM failed to generate a valid script from the file.\n")
#             sys.exit(1)
        
#         print("Story script generated successfully.")

#         # Step 2: Load the video generation pipelines
#         print("Loading video generation models...")
#         image_model_path = os.path.join(os.path.dirname(__file__), 'models', 'juggernautXL_v9.safetensors')
#         image_pipeline = StableDiffusionPipeline.from_single_file(
#             image_model_path,
#             torch_dtype=torch.float16,
#             use_safetensors=True
#         )
#         svd_pipeline = StableVideoDiffusionPipeline.from_pretrained(
#             "stabilityai/stable-video-diffusion-img2vid-xt",
#             torch_dtype=torch.float16,
#             variant="fp16"
#         )
        
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         image_pipeline.to(device)
#         svd_pipeline.to(device)
        
#     except Exception as e:
#         sys.stderr.write(f"Error loading models or generating script: {e}\n")
#         sys.exit(1)

#     all_video_frames = []
#     for i, scene in enumerate(scenes):
#         try:
#             # Generate a realistic starting image for the scene
#             print(f"Generating image for scene {i+1}: {scene['prompt']}")
#             starting_image = image_pipeline(
#                 scene['prompt'],
#                 negative_prompt="blurry, low quality, distorted, bad anatomy",
#                 num_inference_steps=30,
#                 guidance_scale=9.0
#             ).images[0]
            
#             # SVD works best with 1024x576 (landscape)
#             starting_image = starting_image.resize((1024, 576))

#             # Generate video frames for the scene
#             print("Generating video frames...")
#             video_frames = svd_pipeline(
#                 starting_image,
#                 num_frames=25,
#                 decode_chunk_size=8,
#                 motion_bucket_id=140,
#                 noise_aug_strength=0.01
#             ).frames[0]
            
#             all_video_frames.extend(video_frames)
            
#         except Exception as e:
#             sys.stderr.write(f"Error generating video for scene {i+1}: {e}\n")
#             # Continue to the next scene even if one fails
#             continue

#     if not all_video_frames:
#         sys.stderr.write("No video frames were generated.\n")
#         sys.exit(1)

#     # Step 3: Combine all frames into a single video file
#     output_dir = os.path.join(os.path.dirname(__file__), '..', 'storage', 'videos')
#     os.makedirs(output_dir, exist_ok=True)
#     output_filename = f"story_video_{content_id}.mp4"
#     output_path = os.path.join(output_dir, output_filename)
    
#     try:
#         print("Stitching scenes together into a final video...")
#         export_to_video(all_video_frames, output_path, fps=8)
#         print(f"Final video generation complete: storage/videos/{output_filename}")
#         print(f"storage/videos/{output_filename}") # Final path for Node.js
#     except Exception as e:
#         sys.stderr.write(f"Error saving final video: {e}\n")
#         sys.exit(1)

# if __name__ == "__main__":
#     if len(sys.argv) > 2:
#         file_path_arg = sys.argv[1]
#         content_id_arg = sys.argv[2]
#         generate_story_video(file_path_arg, content_id_arg)
#     else:
#         sys.stderr.write("Usage: python story_to_video_gen.py <file_path> <content_id>\n")
#         sys.exit(1)




















# # ml_scripts/story_to_video_gen.py
# import sys
# import os
# import torch
# import json
# from diffusers import StableDiffusionPipeline, StableVideoDiffusionPipeline
# from diffusers.utils import export_to_video
# from PIL import Image
# from transformers import pipeline

# def generate_story_video(file_path, content_id):
#     try:
#         # Step 1: Use LLM to generate a structured script from the file
#         print("Generating story script from file using LLM...")
#         llm_pipeline = pipeline(
#             "text-generation", 
#             model="microsoft/phi-2",
#             device=0,
#             torch_dtype=torch.float16
#         )

#         with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
#             file_content = f.read()

#         llm_prompt = (
#             f"### Instruction:\nBreak down the following story into 3-5 distinct scenes. "
#             f"For each scene, provide a detailed text-to-image prompt. "
#             f"Ensure the prompts are descriptive and follow a cinematic style. "
#             f"Provide the output as a JSON array of objects, with each object "
#             f"containing a 'prompt' key.\n### Story:\n{file_content}\n### Response:\n"
#         )

#         script_output = llm_pipeline(llm_prompt, max_new_tokens=512, do_sample=True)
#         # --- START OF FIX: Robust JSON parsing ---
#         try:
#             # Extract only the JSON part from the LLM's output
#             json_string = script_output[0]['generated_text'].split("### Response:")[1].strip()
#             scenes = json.loads(json_string)
#         except (json.JSONDecodeError, IndexError) as e:
#             sys.stderr.write(f"Error parsing JSON from LLM: {e}\n")
#             sys.exit(1)
#         # --- END OF FIX ---
        
#         if not scenes:
#             sys.stderr.write("LLM failed to generate a valid script from the file.\n")
#             sys.exit(1)
        
#         print("Story script generated successfully.")

#         # Step 2: Load the video generation pipelines
#         print("Loading video generation models...")
#         # --- START OF FIX: Correct function for local models ---
#         image_model_path = os.path.join(os.path.dirname(__file__), 'models', 'juggernautXL_v9.safetensors')
#         image_pipeline = StableDiffusionPipeline.from_single_file(
#             image_model_path,
#             torch_dtype=torch.float16,
#             use_safetensors=True
#         )
#         # --- END OF FIX ---
#         svd_pipeline = StableVideoDiffusionPipeline.from_pretrained(
#             "stabilityai/stable-video-diffusion-img2vid-xt",
#             torch_dtype=torch.float16,
#             variant="fp16"
#         )
        
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         image_pipeline.to(device)
#         svd_pipeline.to(device)
        
#     except Exception as e:
#         sys.stderr.write(f"Error loading models or generating script: {e}\n")
#         sys.exit(1)

#     all_video_frames = []
#     for i, scene in enumerate(scenes):
#         try:
#             # Generate a realistic starting image for the scene
#             print(f"Generating image for scene {i+1}: {scene['prompt']}")
#             starting_image = image_pipeline(
#                 scene['prompt'],
#                 negative_prompt="blurry, low quality, distorted, bad anatomy",
#                 num_inference_steps=30,
#                 guidance_scale=9.0
#             ).images[0]
            
#             # SVD works best with 1024x576 (landscape)
#             starting_image = starting_image.resize((1024, 576))

#             # Generate video frames for the scene
#             print("Generating video frames...")
#             video_frames = svd_pipeline(
#                 starting_image,
#                 num_frames=25,
#                 decode_chunk_size=8,
#                 motion_bucket_id=140,
#                 noise_aug_strength=0.01
#             ).frames[0]
            
#             all_video_frames.extend(video_frames)
            
#         except Exception as e:
#             sys.stderr.write(f"Error generating video for scene {i+1}: {e}\n")
#             continue

#     if not all_video_frames:
#         sys.stderr.write("No video frames were generated.\n")
#         sys.exit(1)

#     # Step 3: Combine all frames into a single video file
#     output_dir = os.path.join(os.path.dirname(__file__), '..', 'storage', 'videos')
#     os.makedirs(output_dir, exist_ok=True)
#     output_filename = f"story_video_{content_id}.mp4"
#     output_path = os.path.join(output_dir, output_filename)
    
#     try:
#         print("Stitching scenes together into a final video...")
#         export_to_video(all_video_frames, output_path, fps=8)
#         print(f"Final video generation complete: storage/videos/{output_filename}")
#         print(f"storage/videos/{output_filename}")
#     except Exception as e:
#         sys.stderr.write(f"Error saving final video: {e}\n")
#         sys.exit(1)

# if __name__ == "__main__":
#     if len(sys.argv) > 2:
#         file_path_arg = sys.argv[1]
#         content_id_arg = sys.argv[2]
#         generate_story_video(file_path_arg, content_id_arg)
#     else:
#         sys.stderr.write("Usage: python story_to_video_gen.py <file_path> <content_id>\n")
#         sys.exit(1)
        
        
        
        
        
        
        
        
# # ml_scripts/story_to_video_gen.py
# import sys
# import os
# import torch
# import json
# from diffusers import StableDiffusionPipeline, StableVideoDiffusionPipeline
# from diffusers.utils import export_to_video
# from PIL import Image
# from transformers import pipeline

# def generate_story_video(file_path, content_id):
#     try:
#         print("Generating story script from file using LLM...")
#         llm_pipeline = pipeline(
#             "text-generation", 
#             model="microsoft/phi-2",
#             device=0,
#             torch_dtype=torch.float16
#         )

#         with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
#             file_content = f.read()
        
#         max_chunk_size = 1500
#         content_chunks = [file_content[i:i+max_chunk_size] for i in range(0, len(file_content), max_chunk_size)]
        
#         scenes = []
#         for chunk in content_chunks:
#             llm_prompt = (
#                 f"### Instruction:\nBreak down the following story into 3-5 distinct scenes. "
#                 f"For each scene, provide a detailed text-to-image prompt. "
#                 f"Ensure the prompts are descriptive and follow a cinematic style. "
#                 f"Provide the output as a JSON array of objects, with each object "
#                 f"containing a 'prompt' key.\n### Story:\n{chunk}\n### Response:\n"
#             )
#             script_output = llm_pipeline(llm_prompt, max_new_tokens=512, do_sample=True)
            
#             # --- START OF FIX ---
#             try:
#                 # Extract only the JSON part from the LLM's output
#                 json_string = script_output[0]['generated_text'].split("### Response:")[1].strip()
#                 scenes.extend(json.loads(json_string))
#             except (json.JSONDecodeError, IndexError) as e:
#                 sys.stderr.write(f"Error parsing JSON from LLM: {e}\n")
#                 continue
#             # --- END OF FIX ---

#         if not scenes:
#             sys.stderr.write("LLM failed to generate a valid script from the file.\n")
#             sys.exit(1)
        
#         print("Story script generated successfully.")

#         # Step 2: Load the video generation pipelines
#         print("Loading video generation models...")
#         image_model_path = os.path.join(os.path.dirname(__file__), 'models', 'juggernautXL_v9.safetensors')
#         image_pipeline = StableDiffusionPipeline.from_single_file(
#             image_model_path,
#             torch_dtype=torch.float16,
#             use_safetensors=True
#         )
#         svd_pipeline = StableVideoDiffusionPipeline.from_pretrained(
#             "stabilityai/stable-video-diffusion-img2vid-xt",
#             torch_dtype=torch.float16,
#             variant="fp16"
#         )
        
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         image_pipeline.to(device)
#         svd_pipeline.to(device)
        
#     except Exception as e:
#         sys.stderr.write(f"Error loading models or generating script: {e}\n")
#         sys.exit(1)

#     all_video_frames = []
#     for i, scene in enumerate(scenes):
#         try:
#             # Generate a realistic starting image for the scene
#             print(f"Generating image for scene {i+1}: {scene['prompt']}")
#             starting_image = image_pipeline(
#                 scene['prompt'],
#                 negative_prompt="blurry, low quality, distorted, bad anatomy",
#                 num_inference_steps=30,
#                 guidance_scale=9.0
#             ).images[0]
            
#             # SVD works best with 1024x576 (landscape)
#             starting_image = starting_image.resize((1024, 576))

#             # Generate video frames for the scene
#             print("Generating video frames...")
#             video_frames = svd_pipeline(
#                 starting_image,
#                 num_frames=25,
#                 decode_chunk_size=8,
#                 motion_bucket_id=140,
#                 noise_aug_strength=0.01
#             ).frames[0]
            
#             all_video_frames.extend(video_frames)
            
#         except Exception as e:
#             sys.stderr.write(f"Error generating video for scene {i+1}: {e}\n")
#             # Continue to the next scene even if one fails
#             continue

#     if not all_video_frames:
#         sys.stderr.write("No video frames were generated.\n")
#         sys.exit(1)

#     # Step 3: Combine all frames into a single video file
#     output_dir = os.path.join(os.path.dirname(__file__), '..', 'storage', 'videos')
#     os.makedirs(output_dir, exist_ok=True)
#     output_filename = f"story_video_{content_id}.mp4"
#     output_path = os.path.join(output_dir, output_filename)
    
#     try:
#         print("Stitching scenes together into a final video...")
#         export_to_video(all_video_frames, output_path, fps=8)
#         print(f"Final video generation complete: storage/videos/{output_filename}")
#         print(f"storage/videos/{output_filename}")
#     except Exception as e:
#         sys.stderr.write(f"Error saving final video: {e}\n")
#         sys.exit(1)

# if __name__ == "__main__":
#     if len(sys.argv) > 2:
#         file_path_arg = sys.argv[1]
#         content_id_arg = sys.argv[2]
#         generate_story_video(file_path_arg, content_id_arg)
#     else:
#         sys.stderr.write("Usage: python story_to_video_gen.py <file_path> <content_id>\n")
#         sys.exit(1)








# # ml_scripts/story_to_video_gen.py
# import sys
# import os
# import torch
# import json
# from diffusers import StableDiffusionPipeline, StableVideoDiffusionPipeline
# from diffusers.utils import export_to_video
# from PIL import Image
# from transformers import pipeline

# def generate_story_video(file_path, content_id):
#     try:
#         # Step 1: Use LLM to generate a structured script from the file
#         print("Generating story script from file using LLM...")
#         llm_pipeline = pipeline(
#             "text-generation", 
#             model="microsoft/phi-2",
#             device=0,
#             torch_dtype=torch.float16
#         )

#         with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
#             file_content = f.read()

#         # The model can't handle long input. Break the content into smaller chunks.
#         # This is a key step to overcoming the model's token limit.
#         max_chunk_size = 1500
#         content_chunks = [file_content[i:i+max_chunk_size] for i in range(0, len(file_content), max_chunk_size)]
        
#         scenes = []
#         for chunk in content_chunks:
#             llm_prompt = (
#                 f"### Instruction:\nBreak down the following story into 3-5 distinct scenes. "
#                 f"For each scene, provide a detailed text-to-image prompt. "
#                 f"Ensure the prompts are descriptive and follow a cinematic style. "
#                 f"Provide the output as a JSON array of objects, with each object "
#                 f"containing a 'prompt' key.\n### Story:\n{chunk}\n### Response:\n"
#             )
#             script_output = llm_pipeline(llm_prompt, max_new_tokens=512, do_sample=True)
#             try:
#                 scenes.extend(json.loads(script_output[0]['generated_text'].split("### Response:")[1].strip()))
#             except json.JSONDecodeError as e:
#                 sys.stderr.write(f"Error parsing JSON from LLM: {e}\n")
#                 continue

#         if not scenes:
#             sys.stderr.write("LLM failed to generate a valid script from the file.\n")
#             sys.exit(1)
        
#         print("Story script generated successfully.")

#         # Step 2: Load the video generation pipelines
#         print("Loading video generation models...")
#         image_model_path = os.path.join(os.path.dirname(__file__), 'models', 'juggernautXL_v9.safetensors')
#         image_pipeline = StableDiffusionPipeline.from_single_file(
#             image_model_path,
#             torch_dtype=torch.float16,
#             use_safetensors=True
#         )
#         svd_pipeline = StableVideoDiffusionPipeline.from_pretrained(
#             "stabilityai/stable-video-diffusion-img2vid-xt",
#             torch_dtype=torch.float16,
#             variant="fp16"
#         )
        
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         image_pipeline.to(device)
#         svd_pipeline.to(device)
        
#     except Exception as e:
#         sys.stderr.write(f"Error loading models or generating script: {e}\n")
#         sys.exit(1)

#     all_video_frames = []
#     for i, scene in enumerate(scenes):
#         try:
#             # Generate a realistic starting image for the scene
#             print(f"Generating image for scene {i+1}: {scene['prompt']}")
#             starting_image = image_pipeline(
#                 scene['prompt'],
#                 negative_prompt="blurry, low quality, distorted, bad anatomy",
#                 num_inference_steps=30,
#                 guidance_scale=9.0
#             ).images[0]
            
#             # SVD works best with 1024x576 (landscape)
#             starting_image = starting_image.resize((1024, 576))

#             # Generate video frames for the scene
#             print("Generating video frames...")
#             video_frames = svd_pipeline(
#                 starting_image,
#                 num_frames=25,
#                 decode_chunk_size=8,
#                 motion_bucket_id=140,
#                 noise_aug_strength=0.01
#             ).frames[0]
            
#             all_video_frames.extend(video_frames)
            
#         except Exception as e:
#             sys.stderr.write(f"Error generating video for scene {i+1}: {e}\n")
#             # Continue to the next scene even if one fails
#             continue

#     if not all_video_frames:
#         sys.stderr.write("No video frames were generated.\n")
#         sys.exit(1)

#     # Step 3: Combine all frames into a single video file
#     output_dir = os.path.join(os.path.dirname(__file__), '..', 'storage', 'videos')
#     os.makedirs(output_dir, exist_ok=True)
#     output_filename = f"story_video_{content_id}.mp4"
#     output_path = os.path.join(output_dir, output_filename)
    
#     try:
#         print("Stitching scenes together into a final video...")
#         export_to_video(all_video_frames, output_path, fps=8)
#         print(f"Final video generation complete: storage/videos/{output_filename}")
#         print(f"storage/videos/{output_filename}") # Final path for Node.js
#     except Exception as e:
#         sys.stderr.write(f"Error saving final video: {e}\n")
#         sys.exit(1)

# if __name__ == "__main__":
#     if len(sys.argv) > 2:
#         file_path_arg = sys.argv[1]
#         content_id_arg = sys.argv[2]
#         generate_story_video(file_path_arg, content_id_arg)
#     else:
#         sys.stderr.write("Usage: python story_to_video_gen.py <file_path> <content_id>\n")
#         sys.exit(1)